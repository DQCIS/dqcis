{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7e94d-1258-47ff-9406-6404c6ee2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['CUDA_HOME'] = '/root/miniconda3/envs/colbert'\n",
    "os.environ['COLBERT_LOAD_TORCH_EXTENSION_VERBOSE'] = 'True'\n",
    "import csv\n",
    "import re\n",
    "import torch\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd22718-ac2b-431f-a12e-268c645e378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(query_file):\n",
    "    queries = []\n",
    "    with open(query_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            query_id, query_text, rewrite_text, condense_text = row\n",
    "            queries.append((query_id, query_text, rewrite_text, condense_text))\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cc271-d3d4-4881-93e4-1c3b2e78d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search(searcher, queries, split_index, split_size, results_dir, top_k, combine_order, threshold, boost_factor):\n",
    "    for query_id, query_text, rewrite_text, condense_text in queries:\n",
    "        results = searcher.coverage_search_for_batch(condense_text, rewrite_text, k=top_k, threshold=threshold, boost_factor=boost_factor, combine_order=combine_order)\n",
    "        topk_pids = [passage_id for passage_id, _, _ in zip(*results)]\n",
    "        original_scores = results[2]\n",
    "        ori_result_file = os.path.join(results_dir, f\"{query_id}.txt\")\n",
    "        with open(ori_result_file, 'a', encoding='utf-8') as ori_file:\n",
    "            for rank, (passage_id, score) in enumerate(zip(topk_pids, results[2]), start=1):\n",
    "                global_passage_id = split_index * split_size + passage_id\n",
    "                ori_file.write(f\"{global_passage_id}\\t{rank}\\t{score}\\n\")\n",
    "            # print(f\"query {query_id} on split {split_index + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0874a-8bad-400b-9a18-d2c1f7cc228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_save_intermediate_results(split_dir, checkpoint_path, experiment_name_prefix, exp_root, queries_list, results_dir_list, top_k, combine_order, threshold, boost_factor, kmeans_niters=4, nbits=2, split_size=2000000):\n",
    "    print(f\"split_dir: {split_dir}\") \n",
    "    split_files = sorted(\n",
    "    [os.path.join(split_dir, f) for f in os.listdir(split_dir) if f.endswith('.tsv')],\n",
    "    key=lambda x: int(os.path.basename(x).split('_')[-1].split('.')[0])\n",
    "    )\n",
    "\n",
    "    for file in split_files:\n",
    "        print(file)\n",
    "        \n",
    "    for split_index in range(len(split_files)):\n",
    "        print(f\"split_dir_1: {split_dir}\")\n",
    "        experiment_name = f\"{experiment_name_prefix}_split_{split_index + 1}\"\n",
    "        index_name = f\"{experiment_name}.nbits={nbits}\"\n",
    "        print(f\"index_name: {index_name}\")  \n",
    "        print(f\"exp_name: {experiment_name}\")\n",
    "        with Run().context(RunConfig(nranks=1, experiment=experiment_name)):\n",
    "            config = ColBERTConfig(nbits=nbits, root=exp_root, kmeans_niters=kmeans_niters)\n",
    "            searcher = Searcher(index=index_name,\n",
    "                                checkpoint=checkpoint_path,\n",
    "                                collection=os.path.join(split_dir, f\"{experiment_name_prefix}_{split_index + 1}.new.tsv\"),\n",
    "                                config=config)\n",
    "            for queries, results_dir in zip(queries_list, results_dir_list):\n",
    "                perform_search(searcher, queries, split_index, split_size, results_dir, top_k, combine_order, threshold, boost_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b430b-d6b0-412c-90c8-b40214cccf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_save_final_results(queries, results_dir, top_k=100):\n",
    "    final_results = {}\n",
    "    for query_index, (query_id, _, _, _) in enumerate(queries):\n",
    "        result_file = os.path.join(results_dir, f\"{query_id}.txt\")\n",
    "        with open(result_file, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        results = [(line.strip().split('\\t')[0], int(line.strip().split('\\t')[1]), float(line.strip().split('\\t')[2])) for line in lines]\n",
    "        results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "        unique_results = []\n",
    "        seen_doc_ids = set()\n",
    "        for result in results:\n",
    "            if result[0] not in seen_doc_ids:\n",
    "                unique_results.append(result)\n",
    "                seen_doc_ids.add(result[0])\n",
    "            if len(unique_results) == top_k * 3: \n",
    "                break\n",
    "        if len(unique_results) < top_k * 3:\n",
    "            for result in results:\n",
    "                if result[0] not in seen_doc_ids:\n",
    "                    unique_results.append(result)\n",
    "                    seen_doc_ids.add(result[0])\n",
    "                if len(unique_results) == top_k * 3:\n",
    "                    break\n",
    "        final_results[query_id] = unique_results[:top_k * 3]\n",
    "    final_result_file = os.path.join(results_dir, 'final_results.txt')\n",
    "    with open(final_result_file, 'w', encoding='utf-8') as file:\n",
    "        for query_id, results in final_results.items():\n",
    "            for rank, (global_passage_id, _, passage_score) in enumerate(results, start=1):\n",
    "                file.write(f\"{query_id}\\tQ0\\t{global_passage_id}\\t{rank}\\t{passage_score}\\tCR\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c10c73-3b30-4d11-a955-7ff1cee23348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_id_mapping(collection_file):\n",
    "    id_mapping = {}\n",
    "    with open(collection_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if len(row) != 2: \n",
    "                continue\n",
    "            original_id, numeric_id = row\n",
    "            if not numeric_id: \n",
    "                continue\n",
    "            id_mapping[numeric_id] = original_id\n",
    "    return id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d0db3-f34c-4cb1-beaa-c91208344749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ids_to_original(results_file, id_mapping, output_file):\n",
    "    with open(results_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split('\\t')\n",
    "            query_id, global_passage_id, rank, score, method = parts[0], parts[2], parts[3], parts[4], parts[5]\n",
    "            original_id = id_mapping.get(global_passage_id, global_passage_id)\n",
    "            outfile.write(f\"{query_id}\\tQ0\\t{original_id}\\t{rank}\\t{score}\\t{method}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27427b9-b018-40f7-81ee-d180884d13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(split_directory, checkpoint_path, experiment_name_prefix, exp_root, queries_dir_list, results_dir_base, default_top_k, default_threshold, default_boost_factor, default_combine_order):\n",
    "    queries_list = []\n",
    "    results_dir_list = []\n",
    "\n",
    "    for query_file in queries_dir_list:\n",
    "        if os.path.isfile(query_file):\n",
    "            queries = read_queries(query_file)\n",
    "            queries_list.append(queries)\n",
    "            print(f\"read query file: {query_file}\")\n",
    "        else:\n",
    "            print(f\" {query_file} does not exist.\")\n",
    "\n",
    "    param_combinations = [\n",
    "        {'top_k': k, 'threshold': default_threshold, 'boost_factor': default_boost_factor, 'combine_order': default_combine_order} for k in top_k_values if k != default_top_k\n",
    "    ] + [\n",
    "        {'top_k': default_top_k, 'threshold': t, 'boost_factor': default_boost_factor, 'combine_order': default_combine_order} for t in thresholds if t != default_threshold\n",
    "    ] + [\n",
    "        {'top_k': default_top_k, 'threshold': default_threshold, 'boost_factor': b, 'combine_order': default_combine_order} for b in boost_factors if b != default_boost_factor\n",
    "    ] + [\n",
    "        {'top_k': default_top_k, 'threshold': default_threshold, 'boost_factor': default_boost_factor, 'combine_order': c} for c in combine_orders if c != default_combine_order\n",
    "    ] + [\n",
    "        {'top_k': default_top_k, 'threshold': default_threshold, 'boost_factor': default_boost_factor, 'combine_order': default_combine_order} \n",
    "    ] \n",
    "\n",
    "    start_index = 0\n",
    "\n",
    "    for i, params in enumerate(param_combinations[start_index:], start=start_index):\n",
    "        results_dir = f\"{results_dir_base}{i + 1}\"\n",
    "        try:\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"error creating directory {results_dir}: {e}\")\n",
    "        \n",
    "        config_file = os.path.join(results_dir, 'config.txt')\n",
    "        with open(config_file, 'w') as f:\n",
    "            f.write(f\"top_k: {params['top_k']}\\n\")\n",
    "            f.write(f\"threshold: {params['threshold']}\\n\")\n",
    "            f.write(f\"boost_factor: {params['boost_factor']}\\n\")\n",
    "            f.write(f\"combine_order: {params['combine_order']}\\n\")\n",
    "        \n",
    "        search_and_save_intermediate_results(\n",
    "            split_directory, checkpoint_path, experiment_name_prefix, exp_root, \n",
    "            queries_list, [results_dir], \n",
    "            params['top_k'], params['combine_order'], params['threshold'], params['boost_factor']\n",
    "        )\n",
    "        for queries in queries_list:\n",
    "            merge_and_save_final_results(queries, results_dir, top_k=100)\n",
    "            print(f\"result saved: {results_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea8625-fee2-4698-b40f-4fd30aa37218",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "    split_directory = \"/split_collection_your_dataset\" # {id}/t{doc_text} The IDs in each split start from 0.\n",
    "    checkpoint_path = \"/ColBERT/colbertv2.0\"\n",
    "    experiment_name_prefix = \"your_dataset\"\n",
    "    exp_root = \"/experiments\"\n",
    "    queries_dir_list = [\"/query/query.tsv\"] # {query_id}/t{raw_query}/t{full-rewrite query}/t{condensed-rewrite query}\n",
    "    results_dir_base = \"/result/results_dir_\" \n",
    "    collection_file = \"/your_dataset.intmapping\"  # {id}/t{realid}\n",
    "\n",
    "    default_top_k = 1000\n",
    "    default_threshold = 0.5\n",
    "    default_boost_factor = 0.05\n",
    "    default_combine_order = 'original_first'\n",
    "\n",
    "    top_k_values = [1000]\n",
    "    thresholds = [0.5]\n",
    "    boost_factors = [0.05]\n",
    "    combine_orders = ['original_first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0c38c-9846-430c-a1df-df8ed284f168",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "    main(split_directory, checkpoint_path, experiment_name_prefix, exp_root, queries_dir_list, results_dir_base, default_top_k, default_threshold, default_boost_factor, default_combine_order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colbert",
   "language": "python",
   "name": "colbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
